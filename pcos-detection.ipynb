{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random, os\n",
    "import numpy as np\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and visualize the data, and create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#train and test data directory\n",
    "data_dir = \"datasetpcos/data/train\"\n",
    "test_data_dir = \"datasetpcos/data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Redefine the 'ImageFolder' class so it won't load the corrupted files\n",
    "\n",
    "class ImageFolderEX(torchvision.datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        try:\n",
    "            sample = self.loader(path)\n",
    "        except:\n",
    "            return None\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset samples: 1924\n",
      "test samples: 1932\n",
      "Classes are: ['infected', 'notinfected']\n",
      "Example: first sample size: torch.Size([3, 256, 256]), and label: 0\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize((256,256)),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "#load the train and test data\n",
    "dataset = ImageFolderEX(data_dir, transform = transform)\n",
    "test_ds = ImageFolderEX(test_data_dir, transform = transform)\n",
    "\n",
    "print('dataset samples:', len(dataset))\n",
    "print('test samples:', len(test_ds))\n",
    "print(\"Classes are:\", dataset.classes)\n",
    "\n",
    "img, label = dataset[0]\n",
    "print(f'Example: first sample size: {img.shape}, and label: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train Data : 1524\n",
      "Length of Validation Data : 400\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "val_size = 400\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Length of Train Data : {len(train_data)}\")\n",
    "print(f\"Length of Validation Data : {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the train and validation into batches\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size, shuffle = True, collate_fn=collate_fn, num_workers = 4, pin_memory = True)\n",
    "val_dl = DataLoader(val_data, batch_size*2, collate_fn=collate_fn, num_workers = 4, pin_memory = True)\n",
    "test_dl = DataLoader(test_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch from dataset\n",
    "images, labels = next(iter(train_dl))\n",
    "print(images.shape , labels.shape)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(images[i,0], cmap=plt.cm.gray)\n",
    "    if labels[i]==0:\n",
    "        plt.title('Infected', fontsize=10)\n",
    "    else:\n",
    "        plt.title('Not Infected', fontsize=10)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the CNN model\n",
    "I used the model from here: <https://medium.com/thecyphy/train-cnn-model-with-pytorch-21dafb918f48>, \n",
    "and the same architecture as Model2 in: <https://www.kaggle.com/code/aykorde/fightos-cnn-models#Model-2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "  \n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, 12, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3,3),\n",
    "            \n",
    "            nn.Conv2d(12, 15, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3,3),\n",
    "        \n",
    "            nn.Conv2d(15, 10, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3,3),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(810, 2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier().to(device)\n",
    "\n",
    "# for name, params in model.named_parameters():\n",
    "#     print('layer name: {}, \\t params shape: {}'.format(name, params.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001\n",
    "\n",
    "#fitting the model on training data and record the result after each epoch\n",
    "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "torch.save(model.state_dict(), 'PCOS_detection_{}_epochs_val_acc_{}.pth'.format(num_epochs, history[-1]['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(history):\n",
    "    \"\"\" Plot the history of accuracies\"\"\"\n",
    "    accuracies = [x['val_acc'] for x in history]\n",
    "    plt.plot(accuracies, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Validation accuracy vs. No. of epochs');\n",
    "\n",
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    \"\"\" Plot the losses in each epoch\"\"\"\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs');\n",
    "\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(model, test_dl)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img_class(img,model):\n",
    "    \"\"\" Predict the class of image and Return Predicted Class\"\"\"\n",
    "    img = img.unsqueeze(0).to(device)\n",
    "    prediction =  model(img)\n",
    "    _, preds = torch.max(prediction, dim = 1)\n",
    "    return dataset.classes[preds[0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = random.choice(os.listdir('/kaggle/input/pcos-detection-using-ultrasound-images/data/test/infected'))\n",
    "img = Image.open(f'/kaggle/input/pcos-detection-using-ultrasound-images/data/test/infected/{test_img}')\n",
    "img = transform(img)\n",
    "plt.imshow(img.permute(1,2,0))\n",
    "\n",
    "pred = predict_img_class(img,model)\n",
    "print(f\"Predicted Class: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the model using SHAP Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CPU for SHAP explainer\n",
    "shap_device = torch.device('cpu')\n",
    "model = model.to(shap_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a batch from train data for background\n",
    "background, labels = next(iter(train_dl))\n",
    "background = background.to(shap_device)\n",
    "\n",
    "#Load a batch from test data\n",
    "test_images, test_labels = next(iter(test_dl))\n",
    "\n",
    "print('background images:', background.shape)\n",
    "print('background labels:', labels.shape)\n",
    "print('test images:', test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the SHAP explainer and get shap values\n",
    "explainer = shap.DeepExplainer(model, background)\n",
    "shap_values = explainer.shap_values(test_images)\n",
    "\n",
    "print('Shap values:', len(shap_values), shap_values[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy and reshape for plotting\n",
    "shap_numpy = list(np.array(shap_values).transpose(0,1,3,4,2))\n",
    "test_numpy = np.array(test_images).transpose(0,2,3,1)\n",
    "\n",
    "print('Shap values, numpy:', len(shap_numpy), shap_numpy[0].shape)\n",
    "print('test images, numpy:', test_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list_labels = list(np.array(test_labels))\n",
    "GT_labels = [dataset.classes[test_list_labels[i].item()] for i in range(len(test_list_labels))]\n",
    "print(GT_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.image_plot(shap_numpy, test_numpy, labels=dataset.classes, true_labels=GT_labels)\n",
    "shap.image_plot(shap_numpy, test_numpy, true_labels=GT_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2065225,
     "sourceId": 3426692,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
